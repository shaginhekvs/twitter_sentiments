{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from preprocessing_merged import *\n",
    "from itertools import combinations_with_replacement, permutations\n",
    "\n",
    "import tflearn as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.layers.recurrent import bidirectional_rnn, BasicLSTMCell\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Varuables\n",
    "logdir='/tmp/tflearn-logs' #Path for the logs\n",
    "n_train=15000 #Number of samples from each of the classes tu use in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_tweet_to_list(positive,full,is_train=True,max_num=None):\n",
    "    '''\n",
    "    Read tweets from the database\n",
    "    INPUT:\n",
    "        positive: boolean, if true, read the positive tweets file\n",
    "        full: boolean, if true, read the tweets file with all tweets\n",
    "        is_train: boolean, if true, read labeled tweets\n",
    "        max_num: integer, maximum number of tweets to read\n",
    "    OUTPUT:\n",
    "        list with tweets read\n",
    "    '''\n",
    "    #Initialise list of tweets\n",
    "    tweets=[] \n",
    "    \n",
    "    #Name of file to read depending on the options\n",
    "    base='train_'\n",
    "    #Positive\n",
    "    if positive:\n",
    "        base+='pos'\n",
    "    #Negative\n",
    "    else:\n",
    "        base+='neg'\n",
    "    #Full database\n",
    "    if full:\n",
    "        base+='_full'\n",
    "    #Without labels    \n",
    "    if(is_train==False):\n",
    "        base='test_data'\n",
    "    #Add extension\n",
    "    base+='.txt'\n",
    "    \n",
    "    #Read the tweets\n",
    "    count=0 #Tweets read so far\n",
    "    max_len=0 #Max length of tweet found\n",
    "    \n",
    "    #Open file\n",
    "    with open('./twitter-datasets/{}'.format(base)) as f:\n",
    "        #Read each line\n",
    "        for line in f:\n",
    "            #Save max tweet length found\n",
    "            if(len(line)>max_len):\n",
    "                max_len=len(line)\n",
    "            #Add tweet to the list removing\n",
    "            tweets.append(''.join(list(filter(lambda x: not x.isdigit(), line))))\n",
    "            \n",
    "            #Increment counter unless we reach the limit\n",
    "            if(max_num):\n",
    "                count+=1\n",
    "                if(count>max_num):\n",
    "                    break\n",
    "    print ('max len of is_positive:{} tweet is {}'.format(positive,max_len))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bidir_LSTM(num_vocab, dropout_bidir=0.7, dropout_lstm=0.8, logdir='/tmp/tflearn_logs/'):\n",
    "    '''\n",
    "    Deep neural net with one branch with bidirectional and one branch with LSTM\n",
    "    INPUT:\n",
    "        num_vocab: number of words in features\n",
    "        dropout_bidir: dropout in bidirectional branch\n",
    "        dropout_lstm: dropout LSTM branch\n",
    "        logdir: path for the logs, should be the same as with the projector\n",
    "    OUTPUT:\n",
    "        model\n",
    "\n",
    "    '''\n",
    "    #Create SentimentalAnalysis Layer(LSTM)\n",
    "    tensorflow.reset_default_graph()\n",
    "    #Input layer\n",
    "    net = tf.input_data([None, 100] ,name='input_layer')\n",
    "    #Embedding\n",
    "    net = tf.embedding(net, input_dim=num_vocab, output_dim=256,name='embedded_layer')\n",
    "    \n",
    "    #Bidirectional branch\n",
    "    net1 = tf.bidirectional_rnn(net,BasicLSTMCell(256), BasicLSTMCell(256),return_seq=True)\n",
    "    if isinstance(net1, list):\n",
    "        net1 = tensorflow.stack(net1, axis=1)\n",
    "    net1 = tf.dropout(net1, dropout_bidir) #Dropout\n",
    "    net1 = tf.fully_connected(net1, 150, activation='relu',name='first_fc',regularizer='L2') #Fully connected layer\n",
    "    \n",
    "    #LTSM branch with dropout\n",
    "    net2 = tf.lstm(net, 256, dropout = dropout_lstm,name = 'LSTM_layer',return_seq=True)\n",
    "    #If 3d array not returned, stack into a 3D array\n",
    "    if isinstance(net2, list):\n",
    "        net2 = tensorflow.stack(net2, axis=1)\n",
    "    net2 = tf.fully_connected(net2, 200, activation='relu',name='second_fc',regularizer='L2') #Fully connected layer\n",
    "    \n",
    "    #Concatenate the results from each branch\n",
    "    net_final=tensorflow.concat([net1,net2],1, name=\"concat\")\n",
    "    #Connect all the cells with softmax\n",
    "    net_final = tf.fully_connected(net_final, 2, activation='softmax',name='output')\n",
    "    #Obtain output with categorical cross entropy\n",
    "    net_final = tf.regression(net_final, optimizer='adam', learning_rate=0.001,\n",
    "                             loss='categorical_crossentropy')\n",
    "    \n",
    "    model = tf.DNN(net_final, tensorboard_verbose=0,checkpoint_path=None, tensorboard_dir=logdir)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tensorboard_projector(features, log_dir='/tmp/tflearn_logs/model/'):\n",
    "    '''\n",
    "    Create the tensorboard projector\n",
    "    INPUT:\n",
    "        features: features returned by divide_train_test\n",
    "        log_dir_path: path to save the logs\n",
    "    '''\n",
    "    # Use the same LOG_DIR where you stored your checkpoint.\n",
    "    summary_writer = tensorflow.summary.FileWriter(log_dir)\n",
    "    \n",
    "    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "    config = projector.ProjectorConfig()\n",
    "    \n",
    "    # You can add multiple embeddings. Here we add only one.\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = 'embedded_layer/W'\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    pd.Series(features).to_csv(log_dir+'metadata.tsv',sep='\\n',index=False,header=False)\n",
    "    \n",
    "    embedding.metadata_path = log_dir+'metadata.tsv'\n",
    "    \n",
    "    # Saves a configuration file that TensorBoard will read during startup.\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len of is_positive:False tweet is 226\n",
      "max len of is_positive:True tweet is 218\n",
      "max len of is_positive:False tweet is 177\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Read the tweets to train ad select 15000 from each classs\n",
    "neg_tweets=read_tweet_to_list(False,False,max_num=None)[:n_train]\n",
    "pos_tweets=read_tweet_to_list(True,False,max_num=None)[:n_train]\n",
    "test_tweets=read_tweet_to_list(False,False,False,max_num=None)\n",
    "\n",
    "num_test_tweets=len(test_tweets)\n",
    "num_train_tweets=len(pos_tweets)+len(neg_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert to dataframe\n",
    "df_neg_tweets=pd.DataFrame({'tweet':neg_tweets,'smile':list(np.zeros(len(neg_tweets)))})\n",
    "df_pos_tweets=pd.DataFrame({'tweet':pos_tweets,'smile':list(np.ones(len(pos_tweets)))})\n",
    "df_test_tweets=pd.DataFrame({'tweet':test_tweets,'smile':list(np.ones(len(test_tweets)))})\n",
    "\n",
    "#Concatenate positive and negative in a single dataframe\n",
    "df_full=pd.concat([df_neg_tweets,df_pos_tweets,df_test_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "#List of possible options\n",
    "opts=['length', 'negative', 'stop', 'antonyms', 'slang']\n",
    "\n",
    "#List of possible values of each of the options\n",
    "b=[True, False]\n",
    "preprocessing_options=[] #List of dictionary with resulting preprocessing options\n",
    "\n",
    "#Obtain all combinations of the options of length opts\n",
    "for c in combinations_with_replacement(b, len(opts)):\n",
    "    #Unique permutations of the possible combinations: [a,a,a] -> [a], [a, a, b] -> [a, a, b], [a,b,a], [b,a,a]    \n",
    "    for p in set(permutations(c)):\n",
    "        #Zip them with the options in a dictionary and append it to a list\n",
    "        preprocessing_options.append(dict(zip(opts,p)))\n",
    "print(len(preprocessing_options))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: 0, {'stop': True, 'length': True, 'slang': True, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 1, {'stop': False, 'length': True, 'slang': True, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 2, {'stop': True, 'length': False, 'slang': True, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 3, {'stop': True, 'length': True, 'slang': False, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 4, {'stop': True, 'length': True, 'slang': True, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 5, {'stop': True, 'length': True, 'slang': False, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 6, {'stop': False, 'length': True, 'slang': False, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 7, {'stop': False, 'length': True, 'slang': True, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 8, {'stop': True, 'length': False, 'slang': False, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 9, {'stop': True, 'length': False, 'slang': True, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 10, {'stop': False, 'length': False, 'slang': True, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 11, {'stop': True, 'length': False, 'slang': False, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 12, {'stop': False, 'length': True, 'slang': False, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 13, {'stop': False, 'length': False, 'slang': True, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 14, {'stop': False, 'length': False, 'slang': False, 'negative': True, 'antonyms': False} \n",
      "Preprocessing done\n",
      "Preprocessing: 15, {'stop': False, 'length': False, 'slang': False, 'negative': False, 'antonyms': False} \n",
      "Preprocessing done\n"
     ]
    }
   ],
   "source": [
    "#For each of the preprocessing options generate the train and validation sets with a 80%-20% splits and pickle the result\n",
    "for n,prep_option in enumerate(preprocessing_options):\n",
    "    print(\"Preprocessing: {}, {} \".format(n, prep_option))\n",
    "    #Divide into train and test set\n",
    "    trainX,trainY,testX,testY,X_test,num_vocab, features=divide_train_test(df_full, num_train_tweets,  prep_option)\n",
    "    #Pickle result\n",
    "    pickle.dump([trainX,trainY,testX,testY,X_test,num_vocab,features],open('train_test_prep_'+str(n),'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: model0\n",
      "Log directory: /tmp/tflearn-logs/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a9b3c83830cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#Training for n_epoch, using 64 samples per step and showing the validation accuracy every 1500 training steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     model.fit(trainX, trainY,validation_set=(testX,testY), run_id='model'+str(n),n_epoch=6, batch_size=64,\n\u001b[1;32m---> 15\u001b[1;33m           show_metric=True,snapshot_step=1500,snapshot_epoch=True)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    214\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                          callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     self.summ_writer = writer_summary(\n\u001b[1;32m--> 262\u001b[1;33m                         self.tensorboard_dir + run_id, self.session.graph)\n\u001b[0m\u001b[0;32m    263\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# TF 0.7\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                     self.summ_writer = writer_summary(\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, logdir, graph, max_queue, flush_secs, graph_def)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \"\"\"\n\u001b[0;32m    308\u001b[0m     \u001b[0mevent_writer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEventFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush_secs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_writer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_logdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgraph_def\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m       \u001b[1;31m# Calling it with both graph and graph_def for backward compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m       \u001b[1;31m# Also export the meta_graph_def in this case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m       \u001b[1;31m# graph may itself be a graph_def due to positional arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[1;34m(self, graph, global_step, graph_def)\u001b[0m\n\u001b[0;32m    184\u001b[0m                       \"or the deprecated `GraphDef`\")\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# Finally, add the graph_def to the summary writer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_write_plugin_assets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36m_add_graph_def\u001b[1;34m(self, graph_def, global_step)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_add_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[0mgraph_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m     \u001b[0mevent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mSerializeToString\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1033\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[0;32m   1034\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[1;32m-> 1035\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m   \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1042\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m   \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[1;34m(self, write_bytes)\u001b[0m\n\u001b[0;32m   1048\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m       \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeRepeatedField\u001b[1;34m(write, value)\u001b[0m\n\u001b[0;32m    754\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m         \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m         \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeRepeatedField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mByteSize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1012\u001b[1;33m       \u001b[0msize\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mFieldSize\u001b[1;34m(map_value)\u001b[0m\n\u001b[0;32m    357\u001b[0m       \u001b[1;31m# obvious way to avoid this within the current design without tons of code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m       \u001b[1;31m# duplication.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m       \u001b[0mentry_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m       \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmessage_sizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpp_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_FieldDescriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCPPTYPE_MESSAGE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m         \u001b[0mcopy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[0mnew_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mMakeSubMessageDefault\u001b[1;34m(message)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0mmessage_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mMakeSubMessageDefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m       result._SetListener(\n\u001b[0;32m    425\u001b[0m           \u001b[0m_OneofListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\adapo\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_byte_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cached_byte_size_dirty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[1;31m# Contains a mapping from oneof field descriptors to the descriptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#Range over the preprocessings options\n",
    "for n in range(len(preprocessing_options)):\n",
    "\n",
    "    #Load the data from this preprocessing option\n",
    "    trainX,trainY,testX,testY,X_test,num_vocab,features = pickle.load(open('./preprocessing_res/train_test_prep_'+str(n),'rb'))\n",
    "    \n",
    "    #Create LSTM model\n",
    "    model=create_bidir_LSTM(num_vocab, logdir=logdir)\n",
    "\n",
    "    tensorboard_projector(features,log_dir=logdir)\n",
    "\n",
    "    #Training for n_epoch, using 64 samples per step and showing the validation accuracy every 1500 training steps\n",
    "    model.fit(trainX, trainY,validation_set=(testX,testY), run_id='model'+str(n),n_epoch=6, batch_size=64,\n",
    "          show_metric=True,snapshot_step=1500,snapshot_epoch=True)\n",
    "    \n",
    "    \n",
    "    model.save(logdir+'/model/model_saved_'+str(n)+'.ckpt')\n",
    "    acc=model.evaluate (testX, testY, batch_size=64)\n",
    "    \n",
    "    \n",
    "    pickle.dump([acc],open('model_prep_'+str(n),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
