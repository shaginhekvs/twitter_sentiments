{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of what we have achieved are explained here. \n",
    "\n",
    "### Make sure you are running this notebook from inside the project_final folder, it's important to do so to visualise some results. \n",
    "#### The results have been given from bottom up manner, i.e. the final results are at the top because they are most important and the first ones at bottom since they might not be very relevant. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Final Model Results\n",
    "## In report : Model 7 - CNN and LSTM: This model is the same as Model 6, but with further pre-preprocessing\n",
    "#### Final model had a very simple topology which had 2 parallel branches, one is CNN and other is LSTM. Key layers are explained below :\n",
    "#### 1. Embeddings: Embedding layer is a lookup and dimensionality reduction on the input. The input matrix has a size of n x 2195885 which is then converted to an embedding size of n x 300 . The matrix decomposition also mantains the spatial correlations between words as mentioned in <sup>[3]</sup>. This embedding matrix is an important feature representation because the semantic relations such as dog -> puppy , thus cat -> kitten are maintained. \n",
    "#### 1. Conv1d layer learns the local regional connections in a window size of 3 which make the model understand the semantic relation of its adjecent neighbours , and the maxpooling kernel after that selects the most representative vector among the three. The output of maxpool is then flattened and then fed to a dense neural network. \n",
    "#### 2. LSTM layer : Recurrent Neural nets have vanishing gradient problem. <sup>[1]</sup> LSTM are able to avoid vanishing gradient problem encountered by Recurrent Neural nets <sup>[2]</sup>and thus can learn hidden states for a long sequence of inputs. We thus expect the LSTM network to be recognise long patterns in the text body which might indicate the sentiments associated with it. LSTM layer is also followed by a dense neural network.\n",
    "#### 3. Merge Layer : The inputs from CNN and LSTM layers are merged to obtain a single layer by matrix concatenation. The merged input is fed to a dense neural network which can then learn the most respresentative features delineating the two classes. \n",
    "#### 4. Output layer : The output layer has a softmax activation function which outputs class probabilites for each class. The maximum of those probabilites is chosen as the final predicted class label. The cost function chosen is categorical class entropy function because it helps avoid training stall which can happen if MSE is chosen as the cost function. Adam optimiser handles backpropogation and learning for us given these parameters.\n",
    "####  We have chosen Relu function as the activation function everywhere to avoid vanishing gradient problem. Positive initialisation of the network weights should ensure non negative inputs to all neurons, thus avoiding dead neuron. \n",
    "\n",
    "\n",
    "#### Both the layers have their own trainable embeddings. The embeddings vectors are of length 300 so that pre trained glove embeddings [glove](https://nlp.stanford.edu/projects/glove/) can be initialised into the plot. The 2 branches are concatenated and the result is fed into a Densely connected Neural network layer which is then finally sent to the output. The tensorflow graph below visualises this very well. In the bottom we have 1 input layer which feeds the input matrix to two parallel layers. \n",
    "!['Graph not found'](./data/images/graph_final.png \"Title\")\n",
    "\n",
    "#### The embedding matrix is trained by the network and the validation losses, training losses, and the related errors are visualised by running the cells below and following the hyperlink upon opening. One can see that the embeddings' PCA shows a clear demarcation which makes sense because these embeddings have been trained by supervised learning, thus backpropagation tries to  create such embeddings which makes the class separation easier. \n",
    "\n",
    "### Image of PCA projection of embeddings' vector trained by CNN branch is below. 3 dimensions capture 34.1% variance in data. There are 94628 features (words) and 300 dimensions. \n",
    "!['PCA1 not found'](./data/images/embedded_layer_pca_cloud.png \"Title\")\n",
    "\n",
    "### Image of PCA projection of embeddings' vector trained by LSTM branch is below. 3 dimensions capture 29.5% variance in data. There are 94628 features (words) and 300 dimensions. \n",
    "!['PCA2 not found'](./data/images/embedded_layer_2_pca_cloud.png \"Title\")\n",
    "\n",
    "### The complete tensorboard can be launched by 2 cells below to visualize everything interactively. Sometimes , to load it properly, please reload the webpage a couple of times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "file_path = \"./data/model_final/checkpoint\"\n",
    "if(os.path.exists(\"./data/model_final/checkpoint\")):\n",
    "    os.remove(\"./data/model_final/checkpoint\")\n",
    "    \n",
    "path_script = os.getcwd()\n",
    "full_path = os.path.join(path_script,'data','model_final','model_saved.ckpt')\n",
    "\n",
    "with open(file_path, \"w\") as f:\n",
    "        f.write('model_checkpoint_path:\"{}\"\\n'.format(full_path))\n",
    "        f.write('all_model_checkpoint_paths:\"{}\"'.format(full_path))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please click on the hyperlink to see the tensorboard visualization\n",
      "TensorBoard 0.4.0rc2 at http://keshav-pc:6006 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "webbrowser.open('http://localhost:6006/#projector')\n",
    "print('Please click on the hyperlink to see the tensorboard visualization')\n",
    "!tensorboard --logdir=\"./data/model_final/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning curves of the final model are shown below. One can hover over them to see exact values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"./data/plots/model 7.html\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "print('learning curves of the final model are shown below. One can hover over them to see exact values')\n",
    "frame = '<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"{}\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>'\n",
    "IPython.display.HTML(frame.format(\"./data/plots/model 7.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model with same architecture as above but without further pre- processing\n",
    "## In report : Model 6 - CNN and LSTM:  It has two neural networks, one is a CNN and another one is a LSTM neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning curves of Model 6 from report are shown below. One can hover over them to see exact values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"./data/plots/model 6.html\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('learning curves of Model 6 from report are shown below. One can hover over them to see exact values')\n",
    "frame = '<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"{}\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>'\n",
    "IPython.display.HTML(frame.format(\"./data/plots/model 6.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.  CNN model with trainable embeddings\n",
    "## Similar to Report Model 5 - CNN with trainable embeddings: This models consists of a Convolutional Neural Network (CNN) , updating the embeddings during training.\n",
    "### The graph of the model is shown below : \n",
    "!['Graph not found'](./data/images/graph_cnn.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning curves of Model 5 from report are shown below. One can hover over them to see exact values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"./data/plots/model 5.html\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('learning curves of Model 5 from report are shown below. One can hover over them to see exact values')\n",
    "frame = '<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"{}\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>'\n",
    "IPython.display.HTML(frame.format(\"./data/plots/model 5.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  CNN model without trainable embeddings\n",
    "## Similar to Report Model 4 - CNN without trainable embeddings: This models consists of a Convolutional Neural Network (CNN) , updating the embeddings during training.\n",
    "### The graph of the model is similar to graph of model 5 : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning curves of Model 4 from report are shown below. One can hover over them to see exact values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"./data/plots/model 4.html\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('learning curves of Model 4 from report are shown below. One can hover over them to see exact values')\n",
    "frame = '<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"{}\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>'\n",
    "IPython.display.HTML(frame.format(\"./data/plots/model 4.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model with parallel LSTM and Bidirectional LSTM layers. \n",
    "\n",
    "## Similar to Report Model 3 - LSTM and bidirectional LSTM: The second model consists of two neural networks, each with their own embedding layer. The first one is a simple LSTM while the second one is a bidirectional LSTM.\n",
    "### We used bidirectional LSTM because it learns the future text context and takes it into consideration before prediction. \n",
    "### The network graph is shown below\n",
    "\n",
    "!['Graph not found'](./data/images/lstm_bi.png \"Title\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning curves of Model 3 from report are shown below. One can hover over them to see exact values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"./data/plots/model 3.html\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('learning curves of Model 3 from report are shown below. One can hover over them to see exact values')\n",
    "frame = '<iframe id=\"themeframe\" width=\"100%\" scrolling=\"no\" src=\"{}\" style=\"display: block; border: none; height: 3654px;\" onload=\"autoResize();\"></iframe>'\n",
    "IPython.display.HTML(frame.format(\"./data/plots/model 3.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "[1]Yoshua Bengio, Patrice Simard, and Paolo Frasconi.1994. Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2):157–166.\n",
    "\n",
    "[2]Sepp Hochreiter and J ̈urgen Schmidhuber. 1997.Long short-term memory. Neural computation,9(8):1735–1780.\n",
    "\n",
    "[3]Tomas Mikolov , Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013 Distributed Representations of Words and Phrases and their Compositionality NIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
