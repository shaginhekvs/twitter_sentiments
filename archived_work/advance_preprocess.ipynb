{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "import logger\n",
    "from logger import NBatchLogger\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook contains code for the advanced preprocessing used for generating expand_join_dict_full. \n",
    "The data has not been provided for this part since it needs 7 GB of glove840B and the pickled input combined, but\n",
    "the algorithm is clear from the flow\n",
    "'''\n",
    "\n",
    "#trainX,trainY,testX,testY,X_test,num_vocab, features=pickle.load(open('train_test_prep_full','rb'))\n",
    "trainX,trainY,testX,testY,X_test,num_vocab, features=pickle.load(open('test_prep_full','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "#f = open( './twitter-datasets/glove.twitter.27B/glove.twitter.27B.200d.txt')\n",
    "#f = open( '/media/keshav/DATA/EPFL/Ubuntu_files/glove.840B.300d.txt')\n",
    "f = open( './twitter-datasets/glove.840B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float64')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        pass\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_char=dict()\n",
    "\n",
    "for char_ in 'abcdefghijklmnopqrstuvwxyz':\n",
    "    mult_char[char_]=char_*3\n",
    "embedding_matrix = np.zeros((num_vocab,300))\n",
    "feature_not_present=[]\n",
    "for i,feature in enumerate(features):\n",
    "    feature=feature.replace('#','') \n",
    "    feature=feature.replace('\"',\"\")\n",
    "    feature=feature.replace(\"'\",\"\")\n",
    "    feature=feature.replace(\"-\",\"\")\n",
    "    feature=feature.replace(\"_\",\"\")\n",
    "    for key,value in mult_char.items():\n",
    "        feature = feature.replace(value,key)\n",
    "    feature=feature.replace('#','') \n",
    "    embedding_vector=embeddings_index.get(feature)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        feature_not_present.append(feature)\n",
    "del(embeddings_index) # free up memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(feature_not_present))\n",
    "slang_dict=pickle.load(open( './twitter-datasets/slang_dict.pkl','rb'))\n",
    "for feature in feature_not_present:\n",
    "    if(feature in slang_dict.keys()):\n",
    "        print(feature + 'is slang for ' + slang_dict[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_segment(text):\n",
    "    probs, lasts = [1.0], [0]\n",
    "    for i in range(1, len(text) + 1):\n",
    "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
    "                        for j in range(max(0, i - max_word_length), i))\n",
    "        probs.append(prob_k)\n",
    "        lasts.append(k)\n",
    "    words = []\n",
    "    i = len(text)\n",
    "    while 0 < i:\n",
    "        words.append(text[lasts[i]:i])\n",
    "        i = lasts[i]\n",
    "    words.reverse()\n",
    "    return words, probs[-1]\n",
    "\n",
    "def word_prob(word): return dictionary[word] / total\n",
    "def words(text): return re.findall('[a-z]+', text.lower()) \n",
    "dictionary = Counter(words(open('./twitter-datasets/words.txt').read()))\n",
    "max_word_length = max(map(len, dictionary))\n",
    "total = float(sum(dictionary.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_expand_lookup={}\n",
    "words_expand_list=[]\n",
    "for feature in feature_not_present:\n",
    "    if(len(feature) >  2):\n",
    "        list_words=viterbi_segment(feature)[0]\n",
    "        words_expand_list.extend(list_words)\n",
    "        words_expand_lookup[feature]=list_words\n",
    "    \n",
    "final_features_not_present=[]\n",
    "for i,feature in enumerate(words_expand_list):\n",
    "    embedding_vector=embeddings_index.get(feature)\n",
    "    if(embedding_vector is not None):\n",
    "        pass\n",
    "    else:\n",
    "        final_features_not_present.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in words_expand_lookup.items():\n",
    "    if '.' in value:\n",
    "        remaining_words=''.join(value[value.index('.')+1:])\n",
    "        words_expand_lookup[key].extend(remaining_words.split('.'))\n",
    "    if( '.' in words_expand_lookup[key] ):\n",
    "        words_expand_lookup[key].remove('.')\n",
    "    if(' ' in words_expand_lookup[key] ):\n",
    "         words_expand_lookup[key].remove(' ')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in final_features_not_present:\n",
    "    if(len(feature) >  0):\n",
    "        print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dict2,open('./twitter-datasets/expand_join_dict_full','wb'))\n",
    "dict1=pickle.load(open('./twitter-datasets/expand_join_dict_test','rb'))\n",
    "dict2=pickle.load(open('./twitter-datasets/expand_join_dict','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in dict1.items():\n",
    "    if(key not in dict2.keys()):\n",
    "        dict2[key]=value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
